# ðŸ“„ Controlling Hallucinations in Large Language Models

This repository contains my research paper:

**"Controlling Hallucinations in Large Language Models"**  

## ðŸ“˜ Abstract
This paper investigates hallucination patterns in Large Language Models (LLMs) using **chain-of-thought reasoning** and **semantic entropy** scores. It proposes and evaluates methods to reduce hallucinations, achieving:
- **18% reduction** in factual errors  
- **22% improvement** in response consistency  
compared to baseline models.

## ðŸ§  Key Contributions
- Analysis of hallucination behavior in LLMs
- Integration of semantic entropy for uncertainty measurement
- Use of chain-of-thought reasoning to improve factual accuracy
- Comparative evaluation against baseline models


